{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO（Proximal Policy Optimization）近端策略优化算法\n",
    "PPO是同策略算法。强化学习（RL）→ 策略梯度方法 → on-policy（在线学习），但通过重要性采样近似实现off-policy（离线学习）的数据复用。PPO 通过约束策略更新幅度，在稳定性和样本效率之间取得平衡，核心思想包括\n",
    "- **剪裁目标函数**  \n",
    "  通过剪裁新旧策略比值，防止策略更新幅度过大导致性能崩溃。  \n",
    "  **剪裁公式**：  \n",
    "  $$\n",
    "  \\mathcal{L}^{CLIP}(\\theta) = \\mathbb{E}_{s,a\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} A^{\\pi_{\\theta_{\\text{old}}}}(s,a), \\text{clip}\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1-\\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_{\\text{old}}}}(s,a) \\right) \\right]\n",
    "  $$\n",
    "  - $ \\epsilon $ 为剪裁范围（如0.2）。\n",
    "\n",
    "- **优势估计**  \n",
    "  使用广义优势估计（GAE）计算优势函数，平衡偏差与方差：  \n",
    "  $$\n",
    "  A_t^{\\text{GAE}} = \\sum_{k=0}^{T-t-1} (\\gamma\\lambda)^k \\delta_{t+k}, \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "  $$\n",
    "  - $ \\gamma $ 为折扣因子，$ \\lambda $ 为GAE参数。\n",
    "\n",
    "- **策略稳定性约束**  \n",
    "  通过KL散度惩罚项或约束，强制新策略与旧策略的差异不超过阈值：  \n",
    "  $$\n",
    "  \\mathcal{L}^{KL}(\\theta) = \\beta \\cdot \\mathbb{E}_{s\\sim\\pi_{\\theta_{\\text{old}}}} \\left[ D_{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s) || \\pi_{\\theta}(\\cdot|s)) \\right]\n",
    "  $$\n",
    "\n",
    "\n",
    "### **算法流程**\n",
    "1. **数据收集**：使用旧策略 $ \\pi_{\\theta_{\\text{old}}} $ 采样轨迹数据。\n",
    "2. **优势计算**：基于采样数据计算GAE优势 $ A_t^{\\text{GAE}} $。\n",
    "3. **策略更新**：最大化剪裁后的目标函数 $ \\mathcal{L}^{CLIP} $，并通过KL散度约束防止策略突变。\n",
    "4. **价值函数优化**：单独训练Critic网络（如用均方误差优化 $ V(s) $。\n",
    "\n",
    "### **特点**\n",
    "- **稳定性**：剪裁机制和KL约束避免策略崩溃。\n",
    "- **样本效率**：通过重要性采样复用旧数据，减少对新样本的依赖。\n",
    "- **实现简单**：相比TRPO（Trust Region Policy Optimization），无需复杂的二阶优化。\n",
    "- **通用性**：适用于连续/离散动作空间，在游戏AI（如Dota 2）、机器人控制等领域表现优异。\n",
    "\n",
    "### **公式总结**\n",
    "- **总目标函数**：  \n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = \\mathcal{L}^{CLIP}(\\theta) - \\mathcal{L}^{KL}(\\theta)\n",
    "  $$\n",
    "- **优势函数**：GAE公式（见核心原理部分）。\n",
    "\n",
    "\n",
    "### **应用场景**\n",
    "- 复杂游戏AI（如OpenAI Five）。\n",
    "- 机器人运动控制（如四足机器人平衡）。\n",
    "- 自然语言处理（如文本生成优化）。\n",
    "\n",
    "**关键公式**：剪裁目标函数、GAE优势估计、KL散度约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序实现\n",
    "\n",
    "代码中要注意实现的环境不同，输入和输出也不同，对应的超参数设置也不同，例如CarPole-v1的输入是4维连续向量，输出是2维，而CliffWalking-v0的输入是48维one-hot编码，输出是4维离散动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用相关的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型定义\n",
    "\n",
    "##### 定义演员网络和评论家网络\n",
    "\n",
    "PPO算法的model就是actor和critic两个网络。分别用MLP来拟合。Actor输出的是一个概率分布，critic输出的是一个值。critic网络的输入维度也可以是n_states+n_actions，也即将action的信息也纳入critic网络中，这样会更好一些。这里actor拟合**策略网络**，critic拟合**价值函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        return dist\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义经验回放\n",
    "\n",
    "**重要性采样**是从评论员采样到的数据进行重要性采样来更新演员模型。需要一个经验回放的队列，这个队列中存储的是状态、动作、奖励、下一个状态、是否结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferQue:\n",
    "    '''DQN的经验回放池，每次采样batch_size个样本\n",
    "    '''\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        '''初始化经验回放池\n",
    "        Args:\n",
    "            capacity (int): 经验回放池的最大容量\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        # 使用deque实现经验回放池，设置最大长度为capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transitions):\n",
    "        '''将一个经验样本添加到回放池中\n",
    "        Args:\n",
    "            transitions (tuple): 包含(state, action, log_p, reward, done)的经验样本\n",
    "        '''\n",
    "        # 将经验样本添加到buffer中，如果buffer已满，最旧的样本将被移除\n",
    "        self.buffer.append(transitions)\n",
    "    \n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        '''从经验回放池中随机采样一个batch的样本\n",
    "        Args:\n",
    "            batch_size (int): 采样的样本数量\n",
    "            sequential (bool): 是否按照顺序采样，默认为False\n",
    "        Returns:\n",
    "            batch (tuple): 包含(state, action, log_p, reward, done)的batch样本\n",
    "        '''\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential:\n",
    "            # 如果需要按照顺序采样，则从buffer中随机选择batch_size个索引\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            # 根据索引从buffer中取出对应的样本\n",
    "            batch = [self.buffer[idx] for idx in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else:\n",
    "            # 如果不需要按照顺序采样，则直接从buffer中随机选择batch_size个样本\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch) # 解包，返回各部分批量数据的元组\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''返回回放池中样本的数量\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PGReplay(ReplayBufferQue):\n",
    "    '''PG的经验回放池，每次采样所有样本，因此只需要继承ReplayBufferQue，重写sample方法即可\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.buffer = deque()\n",
    "    \n",
    "    def sample(self):\n",
    "        '''从经验回放池中采样所有样本\n",
    "        Returns:\n",
    "            batch (tuple): 包含(state, action, log_p, reward, done)的batch样本\n",
    "        '''\n",
    "        batch = list(self.buffer)\n",
    "        return zip(*batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义PPO智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, cfg) -> None:\n",
    "        self.gamma = cfg.gamma # 折扣因子\n",
    "        self.device = cfg.device # 设备\n",
    "        self.actor = Actor(n_states=cfg.n_states, n_actions=cfg.n_actions, hidden_dim=cfg.hidden_dim).to(self.device)\n",
    "        self.critic = Critic(n_states=cfg.n_states, hidden_dim=cfg.hidden_dim).to(self.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
    "        self.memory = PGReplay() # 记忆回放\n",
    "        self.k_epochs = cfg.k_epochs # 更新策略网络的轮数\n",
    "        self.eps_clip = cfg.eps_clip # PPO的截断参数\n",
    "        self.entropy_coef = cfg.entropy_coef # 熵系数\n",
    "        self.sample_count = 0 # 记录采样次数\n",
    "        self.update_freq = cfg.update_freq # 更新策略网络的频率\n",
    "\n",
    "        def sample_action(self, state):\n",
    "            '''根据当前状态采样动作\n",
    "            Args:\n",
    "                state: 当前环境状态，可以是连续的向量或离散的值。  \n",
    "            Returns:\n",
    "                action: 采样的动作，基于当前策略。\n",
    "            '''\n",
    "            # 每调用一次sample_action，样本计数增加1\n",
    "            self.sample_count += 1\n",
    "            # 输入时连续的向量，需要将其转换为tensor，如果是离散输入的话，需要转换为one-hot向量\n",
    "            state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "            # 使用actor网络计算动作概率分布\n",
    "            probs = self.actor(state)\n",
    "            # 创建一个分类分布对象，便于之后采样动作\n",
    "            dist = Categorical(probs)  \n",
    "            # 从概率分布中采样得到具体动作\n",
    "            action = dist.sample() \n",
    "            # 计算所选动作的对数概率，并从计算图中分离，防止梯度更新时对其产生影响\n",
    "            self.log_probs = dist.log_prob(action).detach()\n",
    "            # 将动作从tensor转换为numpy数组，并返回其第一个元素，作为实际执行的动作\n",
    "            return action.detach().cpu().numpy().item()\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def predict_action(self, state):\n",
    "            state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "            probs = self.actor(state)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            return action.detach().cpu().numpy().item()\n",
    "        \n",
    "        def update(self,):\n",
    "            '''PPO更新策略网络\n",
    "            '''\n",
    "            # 每n次更新一次策略网络\n",
    "            if self.sample_count % self.update_freq != 0:\n",
    "                return\n",
    "            print(\"update policy network\")\n",
    "            old_states, old_actions, old_log_probs, old_rewards, old_dones = self.memory.sample()\n",
    "            # 转换数据为tensor\n",
    "            old_states = torch.tensor(old_states, device=self.device, dtype=torch.float32)\n",
    "            old_actions = torch.tensor(old_actions, device=self.device, dtype=torch.long)\n",
    "            old_log_probs = torch.tensor(old_log_probs, device=self.device, dtype=torch.float32)\n",
    "\n",
    "            # 状态奖励的monte carlo估计\n",
    "            returns = [] # 存储每个状态的累计奖励\n",
    "            discounted_sum = 0 # 折扣累计和\n",
    "            # 逆序遍历旧的奖励和完成标志，计算每个状态的折扣累计奖励\n",
    "            for reward, done in zip(reversed(old_rewards), reversed(old_dones)):\n",
    "                # 如果遇到完成标志（done为True），则重置折扣累计和为0\n",
    "                # 这是因为在一个episode结束时，累积奖励应当重新计算\n",
    "                if done:\n",
    "                    discounted_sum = 0\n",
    "                # 根据当前奖励和折扣因子更新折扣累计和\n",
    "                # 这一步是monte carlo方法中计算回报的关键步骤\n",
    "                discounted_sum = reward + self.gamma * discounted_sum\n",
    "                # 将计算得到的折扣累计和插入到returns列表的最前面\n",
    "                # 以保持与原始状态序列的顺序一致\n",
    "                returns.insert(0, discounted_sum)\n",
    "            \n",
    "            # 归一化奖励\n",
    "            returns = torch.tensor(returns, device=self.device, dtype=torch.float32)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "            for _ in range(self.k_epochs):\n",
    "                # 计算广义优势估计GAE = Q(s,a) - V(s)\n",
    "                values = self.critic(old_states) # detach防止通过critic反传\n",
    "                advantages = returns - values.detach()\n",
    "\n",
    "                # 获取旧策略动作概率\n",
    "                probs = self.actor(old_states)\n",
    "                dist = Categorical(probs)\n",
    "                # 获取新策略的动作概率\n",
    "                new_probs = dist.log_prob(old_actions)\n",
    "\n",
    "                # 计算比值(pi_theta / pi_theta_old)\n",
    "                ratio = (new_probs - old_log_probs).exp()\n",
    "\n",
    "                # 计算替代损失\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "                # 计算actor损失：剪裁机制+熵正则化（鼓励策略探索性，防止过早收敛到次优解）\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() + self.entropy_coef * dist.entropy().mean()\n",
    "                # 计算critic损失\n",
    "                critic_loss = (returns - values).pow(2).mean()\n",
    "                \n",
    "                # 更新策略网络\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                critic_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "            self.memory.clear()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', type=str, default=\"cpu\", help='设备')\n",
    "    parser.add_argument('--algo_name', type=str, default='PPO', help='算法名称')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v0', help='环境名称')\n",
    "    parser.add_argument('--render', type=bool, default=False, help='是否显示环境')\n",
    "\n",
    "    parser.add_argument('--actor_lr', type=float, default=3e-4, help='学习率')\n",
    "    parser.add_argument('--critic_lr', type=float, default=3e-4, help='学习率')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n",
    "    parser.add_argument('--train_eps', type=int, default=10000, help='训练次数')\n",
    "    parser.add_argument('--test_eps', type=int, default=100, help='测试次数')\n",
    "    parser.add_argument('--max_steps', type=int, default=200, help='每个回合的最大步数')\n",
    "    parser.add_argument('--eval_eps', type=int, default=10, help='评估次数')\n",
    "    parser.add_argument('--eval_per_episode', type=int, default=10, help='评估频率')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=8, help='随机种子')\n",
    "    parser.add_argument('--if_load_ckpt', type=bool, default=False, help='是否加载模型')\n",
    "    parser.add_argument('--ckpt_path', type=str, default=\"ckpt/\", help='模型保存路径')\n",
    "    parser.add_argument('--ckpt_name', type=str, default=\"ppo.pth\", help='模型保存名称')\n",
    "    \n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣因子')\n",
    "    parser.add_argument('--n_states', type=int, default=4, help='状态个数')\n",
    "    parser.add_argument('--n_actions', type=int, default=2, help='动作个数')\n",
    "    parser.add_argument('--actor_hidden_dim', type=int, default=256, help='actor隐藏层维度')\n",
    "    parser.add_argument('--critic_hidden_dim', type=int, default=256, help='critic隐藏层维度')\n",
    "    parser.add_argument('--max_steps', type=int, default=200, help='最大步数')\n",
    "    parser.add_argument('--eps_clip', type=float, default=0.2, help='截断阈值')\n",
    "    parser.add_argument('--K_epochs', type=int, default=5, help='PPO更新策略网络的次数')\n",
    "    parser.add_argument('--entropy_coef', type=float, default=0.01, help='熵系数')\n",
    "    parser.add_argument('--update_freq', type=int, default=100, help='更新频率')\n",
    "    \n",
    "\n",
    "    return parser.parse_args([])\n",
    "\n",
    "cfg = get_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(algo_name='PPO', env_name='CartPole-v0', render=False, lr=0.0001, gamma=0.99, batch_size=64, train_epochs=10000, test_epochs=100, ckpt_path='ckpt/', ckpt_name='ppo.pth', device='cpu', n_states=4, n_actions=2, hidden_dim=128, max_steps=200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
