{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 策略梯度\n",
    "最基础的策略梯度算法就是REINFORCE算法，又称Monte-Carlo Gradient算法。我们策略优化的目标如下：\n",
    "$$\n",
    "J_\\theta = \\Psi_\\pi \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "其中$\\Psi_\\pi$在REINFORCE算法中表示衰减的回报。也可以用优势来估计，也就是我们熟知的A3C算法。\n",
    "我们介绍一下策略梯度中最简单的也是最经典的一个算法 REINFORCE。REINFORCE 用的是回合更新的方式，它在代码上的处理上是先获取每个步骤的奖励，然后计算每个步骤的未来总奖励 $G_t$，将每个 $G_t$ 代入\n",
    "$$\n",
    "\\nabla\\bar{R}_\\theta\\approx\\frac{1}{N}\\sum_{n = 1}^{N}\\sum_{t = 1}^{T_n}G_t^n\\nabla\\log\\pi_\\theta(a_t^n|s_t^n)\n",
    "$$ (4.21)\n",
    "\n",
    "优化每一个动作的输出。所以我们在编写代码时会设计一个函数，这个函数的输入是每个步骤获取的奖励，输出是每一个步骤的未来总奖励。因为未来总奖励可写为\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t&=\\sum_{k = t + 1}^{T}\\gamma^{k - t - 1}r_k\\\\\n",
    "&=r_{t + 1}+\\gamma G_{t + 1}\n",
    "\\end{align*}\n",
    "$$ (4.22)\n",
    "\n",
    "即上一个步骤和下一个步骤的未来总奖励的关系如式 (4.22) 所示，所以在代码的计算上，我们是从后往前推，一步一步地往前推，先算 $G_T$，然后往前推，一直算到 $G_1$。\n",
    "\n",
    "如图 4.14 所示，REINFORCE 的伪代码主要看最后 4 行，先产生一个回合的数据，比如\n",
    "$$\n",
    "(s_1,a_1,G_1),(s_2,a_2,G_2),\\cdots,(s_T,a_T,G_T)\n",
    "$$\n",
    "\n",
    "然后针对每个动作计算梯度 $\\nabla\\log\\pi(a_t|s_t,\\theta)$。在代码上计算时，我们要获取神经网络的输出。神经网络会输出每个动作对应的概率值（比如 0.2、0.5、0.3），然后我们还可以获取实际的动作 $a_t$，把动作转成独热（one - hot）向量（比如 $[0,1,0]$）与 $\\log[0.2,0.5,0.3]$ 相乘就可以得到 $\\log\\pi(a_t|s_t,\\theta)$。 \n",
    "\n",
    "## 策略函数设计\n",
    "策略梯度算法是直接对策略函数进行梯度计算，那么策略函数的设计就显得关键了。一般有两种设计方式，一种是softmax函数，另外一个是高斯分布 $\\mathbb{N}(\\phi(\\mathbb{s})^{\\pi}\\theta,\\sigma^2)$，前者用于离散动作空间，后者多用于连续动作空间。\n",
    "\n",
    "softmax函数可以表示为：\n",
    "\n",
    "$$\n",
    "\\pi(a|\\mathbb{s}) = \\frac{e^{\\phi(\\mathbb{s})^{T_\\theta}}}{\\sum_{b} e^{\\phi(s,b)^{T_\\theta}}}\n",
    "$$\n",
    "\n",
    "其中，$\\phi(s)^{\\pi}\\theta$为策略函数，$\\theta$为策略函数的参数，$\\sigma^2$为噪声方差。对应的梯度为\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(s, a)=\\phi(s, a)-\\mathbb{E}_{\\pi_\\theta}[\\phi(s,a)]\n",
    "$$\n",
    "高斯分布对应的梯度为：\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(s, a)=\\frac{\\left(a-\\phi(s)^T \\theta\\right) \\phi(s)}{\\sigma^2}\n",
    "$$\n",
    "但是对于一些特殊的情况，例如在本次演示中动作维度=2且为离散空间，这个时候可以用伯努利分布来实现，这种方式其实是不推荐的，这里给大家做演示也是为了展现一些特殊情况，启发大家一些思考，例如Bernoulli，Binomial，Gaussian分布之间的关系。简单说来，Binomial分布，$n=1$ 时就是Bernoulli分布，$n\\rightarrow \\infty$ 时就是Gaussian分布."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计\n",
    "前面讲到，尽管本次演示是离散空间，但是由于动作维度等于2，此时就可以用特殊的高斯分布来表示策略函数，即伯努利分布。伯努利的分布实际上是用一个概率作为输入，然后从中采样动作，伯努利采样出来的动作只可能是0或1，就像投掷出硬币的正反面。在这种情况下，我们的策略模型就需要在MLP的基础上，将状态作为输入，将动作作为倒数第二层输出，并在最后一层增加激活函数来输出对应动作的概率。不清楚激活函数作用的同学可以再看一遍深度学习相关的知识，简单来说其作用就是增加神经网络的非线性。既然需要输出对应动作的概率，那么输出的值需要处于0-1之间，此时sigmoid函数刚好满足我们的需求，实现代码参考如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Bernoulli\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import sys\n",
    "import os\n",
    "#from utils import *\n",
    "#from utils import smooth, plot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义策略网络pi_\\theta\n",
    "class PGNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        \"\"\"Args:\n",
    "            初始化Q网络，为全连接网络\n",
    "            input_dim: 输入的特征数即环境的状态维度\n",
    "            output_dim: 输出的动作维度\n",
    "        \"\"\"\n",
    "        super(PGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            #nn.sigmoid()\n",
    "        )\n",
    "        #self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = torch.sigmoid(self.fc3(x))\n",
    "        return torch.softmax(self.net(x))\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_file):\n",
    "        torch.save(self.state_dict(), checkpoint_file, _use_new_zipfile_serialization=False)\n",
    " \n",
    "    def load_checkpoint(self, checkpoint_file):\n",
    "        self.load_state_dict(torch.load(checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新函数设计\n",
    "对于梯度我们需要拆开成两个部分$\\Psi_\\pi$和$\\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)$分开计算，首先看值函数部分$\\Psi_\\pi$，在REINFORCE算法中值函数是从当前时刻开始的衰减回报，如下：\n",
    "$$\n",
    "G \\leftarrow \\sum_{k = t + 1}^{T} \\gamma^{k - 1}r_k\n",
    "$$\n",
    "这个实际用代码来实现的时候可能有点绕，我们可以倒过来看，在同一回合下，我们的终止时刻是$T$，那么对应的回报$G_T = \\gamma^{T - 1}r_T$，而对应的$G_{T - 1} = \\gamma^{T - 2}r_{T - 1} + \\gamma^{T - 1}r_T$，在这里代码中我们使用了一个动态规划的技巧，如下：\n",
    "\n",
    "```python\n",
    "running_add = running_add * self.gamma + reward_pool[i]  # running_add初始值为0\n",
    "```\n",
    "\n",
    "这个公式也是倒过来循环的，第一次的值等于：\n",
    "$$\n",
    "running\\_add = r_T\n",
    "$$\n",
    "第二次的值则等于：\n",
    "$$\n",
    "running\\_add = r_T * \\gamma + r_{T - 1}\n",
    "$$\n",
    "第三次的值等于：\n",
    "$$\n",
    "running\\_add = (r_T * \\gamma + r_{T - 1}) * \\gamma + r_{T - 2} = r_T * \\gamma^2 + r_{T - 1} * \\gamma + r_{T - 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    #def __init__(self, model, memory, cfg):\n",
    "    def __init__(self, model, cfg):\n",
    "        self.gamma  = cfg.gamma # 折扣因子\n",
    "        self.device = cfg.device\n",
    "        #self.memory = memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.checkpoint_dir = cfg.ckpt_dir\n",
    "        if cfg.if_load_ckpt:\n",
    "            self.model = model.load_checkpoint(self.checkpoint_dir + sorted(os.listdir(self.checkpoint_dir))[-1])\n",
    "        else:\n",
    "            self.policy_net = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.n_actions = cfg.n_actions\n",
    "        self.n_states = cfg.n_states\n",
    "        \n",
    "    \n",
    "\n",
    "    def sample_action(self, state):\n",
    "        \"\"\"从当前状态采样动作\n",
    "        Args:\n",
    "            state: 状态\"\"\"\n",
    "        #state = torch.from_numpy(state).float() # 将numpy数组转换为PyTorch张量\n",
    "        state = F.one_hot(torch.tensor(state), num_classes=self.n_states).float().to(self.device)\n",
    "        #state = Variable(state) # 包装张量，使其能够记录操作并支持梯度计算\n",
    "        #print(state)\n",
    "        probs = self.policy_net(state) # 输入状态，输出动作概率\n",
    "        m = Categorical(probs) # 建立一个分布\n",
    "        action = m.sample() # 采样一个动作\n",
    "        self.log_probs.append(m.log_prob(action)) # 保存log概率用于梯度计算\n",
    "\n",
    "        #action = action.data.numpy().astype(int)[0] # 转化为标量\n",
    "\n",
    "        return action.item()\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        \"\"\"从当前状态预测下一步的动作\n",
    "        \"\"\"\n",
    "        #state = torch.from_numpy(state).float()\n",
    "        state = F.one_hot(torch.tensor(state, dtype=int), num_classes=self.n_states).int()\n",
    "        state = Variable(state)\n",
    "        probs = self.policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        action = action.data.numpy().astype(int)[0]\n",
    "        #?????和采样动作有区别吗？\n",
    "        return action\n",
    "    \n",
    "    def save_models(self, episode):\n",
    "        self.policy_net.save_checkpoint(self.checkpoint_dir + 'Reinforce_policy_{}.pth'.format(episode))\n",
    "        print('Saved the policy network successfully!')\n",
    "\n",
    "    def load_models(self, episode):\n",
    "        self.policy_net.load_checkpoint(self.checkpoint_dir + 'Reinforce_policy_{}.pth'.format(episode))\n",
    "        print('Loaded the policy network successfully!')\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"train_step()\n",
    "        \"\"\"\n",
    "        #state_pool, action_pool, reward_pool = self.memory.sample()\n",
    "        #state_pool, action_pool, reward_pool = list(state_pool), list(action_pool), list(reward_pool)\n",
    "        \n",
    "        reward_pool = []\n",
    "        \n",
    "        # 折扣奖励\n",
    "        running_add = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            running_add = running_add * self.gamma + r\n",
    "            reward_pool.insert(0, running_add)\n",
    "        \n",
    "        # 归一化\n",
    "        rewards_normed = torch.tensor(reward_pool)\n",
    "        rewards_normed = (rewards_normed - rewards_normed.mean()) / (rewards_normed.std() + 1e-7)\n",
    "        '''reward_mean = np.mean(reward_pool)\n",
    "        reward_std = np.std(reward_pool)\n",
    "        for i in range(len(reward_pool)):\n",
    "            reward_pool[i] = (reward_pool[i] - reward_mean) / (reward_std + 1e-7)\n",
    "        '''\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, reward in zip(self.log_probs, rewards_normed):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        \n",
    "        # 梯度下降\n",
    "        self.optimizer.zero_grad()\n",
    "        '''\n",
    "        for i in range(len(self.rewards)):\n",
    "            #state = state_pool[i]\n",
    "            #action = Variable(torch.FloatTensor([action_pool[i]]))\n",
    "            reward = reward_pool[i]\n",
    "            #state = Variable(torch.from_numpy(state).float())\n",
    "            #probs = self.policy_net(state)\n",
    "            #m = Bernoulli(probs)\n",
    "            #loss = -m.log_prob(action) * reward #\n",
    "            loss = -self.log_probs[i] * reward\n",
    "            loss.backward()\n",
    "        '''\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #self.memory.clear()\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练环境配置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    '''\n",
    "    - cfg: 配置参数\n",
    "    - env: 环境\n",
    "    - agent: 算法\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    print(f'环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}')\n",
    "    rewards = [] # 记录奖励\n",
    "    losses = []\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0 # 一轮episode的reward\n",
    "        state = env.reset() # 重置环境, 重新开一局（即开始新的一个episode）\n",
    "        #action = agent.sample(state)\n",
    "        while True:\n",
    "            #print(state)\n",
    "            action = agent.sample_action(state) # 根据算法采样一个动作\n",
    "            #print(f\"状态:{state}, 动作:{action}\")\n",
    "            next_state, reward, done, _ = env.step(action) # 与环境进行一个交互\n",
    "            #print(next_state, done)\n",
    "            #next_action = agent.sample_action(next_state)\n",
    "            #agent.update(state, action, reward, next_state, next_action, done) # Sarsa算法更新\n",
    "            #agent.update()\n",
    "            #state = next_state\n",
    "            #action = next_action\n",
    "            agent.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            #ep_reward += reward\n",
    "            if done:\n",
    "                print(\"一次游戏结束.\")\n",
    "                break\n",
    "        \n",
    "        # 训练并记录数据\n",
    "        loss = agent.update()\n",
    "        losses.append(loss)\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "        if (i_ep+1) % 10 == 0: # 每50个回合打印一次信息\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{np.mean(rewards):.1f}，损失：{loss:.3f}\")\n",
    "\n",
    "        if (i_ep+1) % 50 == 0:\n",
    "            agent.save_models(i_ep+1)\n",
    "    print(\"完成训练！\")\n",
    "    return {\"rewards\": rewards}\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    print(f\"环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}\")\n",
    "\n",
    "    rewards = [] # 记录所有回合的episode奖励\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        ep_reward = 0 # 一轮episode的reward\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            #print(i_ep)\n",
    "            action = agent.predict_action(state) # 根据算法选择一个动作\n",
    "            next_state, reward, done, _ = env.step(action) # 与环境进行一个交互\n",
    "            state = next_state # 更新状态\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.1f}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {\"rewards\": rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境和智能体配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg):\n",
    "    \"\"\"创建环境和智能体\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg.env_name)\n",
    "    #env = CliffWalkingWrapper(env)\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    cfg.input_dim = n_states\n",
    "    cfg.output_dim = n_actions\n",
    "    cfg.n_states = n_states\n",
    "    cfg.n_actions = n_actions\n",
    "    agent = PolicyGradient(\n",
    "        model=PGNet(input_dim=cfg.input_dim, \n",
    "                    output_dim=cfg.output_dim,\n",
    "                    hidden_dim=cfg.hidden_dim), \n",
    "        cfg=cfg)\n",
    "    return env, agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"配置参数\n",
    "    \"\"\"\n",
    "    curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    parser = argparse.ArgumentParser(description=\"hyperpaparameter\")\n",
    "    parser.add_argument(\"--algo_name\", default=\"PolicyGradient\", type=str, help=\"算法名称\")\n",
    "    parser.add_argument(\"--env_name\", default=\"CliffWalking-v0\", type=str, help=\"环境名称\")\n",
    "    parser.add_argument(\"--train_eps\", default=400, type=int, help=\"训练回合数\")\n",
    "    parser.add_argument(\"--test_eps\", default=20, type=int, help=\"测试回合数\")\n",
    "    parser.add_argument(\"--input_dim\", default=4, type=int, help=\"状态维度\")\n",
    "    parser.add_argument(\"--hidden_dim\", default=128, type=int, help=\"隐藏层维度\")\n",
    "    parser.add_argument(\"--n_actions\", default=4, type=int, help=\"动作维度\")\n",
    "    parser.add_argument(\"--n_states\", default=48, type=int, help=\"状态维度\")\n",
    "    parser.add_argument(\"--output_dim\", default=2, type=int, help=\"动作维度\")\n",
    "    parser.add_argument(\"--gamma\", default=0.9, type=float, help=\"折扣因子\")\n",
    "    parser.add_argument(\"--lr\", default=0.001, type=float, help=\"学习率\")\n",
    "    parser.add_argument(\"--if_load_ckpt\", default=False, type=bool, help=\"是否加载模型\")\n",
    "    parser.add_argument(\"--ckpt_dir\", default=\"/kaggle/working/\", type=str, help=\"模型存储路径\")\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", type=str, help=\"cpu或者gpu\")\n",
    "    parser.add_argument(\"--seed\", default=8, type=int, help=\"随机种子\")\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "def smooth(data, weigth=0.9):\n",
    "    \"\"\"用于平滑曲线，类似于Tensorboard中的smooth\n",
    "    \n",
    "    Args:\n",
    "        data(List): 用于平滑的数组\n",
    "        weigth(Float): 平滑权重，0-1之间，值越大越平滑，一般取0.9\n",
    "    \n",
    "    Returns:\n",
    "        smoothed(List): 平滑后的数组\n",
    "    \"\"\"\n",
    "    last = data[0] # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in data:\n",
    "        smoothed_val = last * weigth + (1 - weigth) * point # 计算平滑值\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val # Array is smoothed so the last value is smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards, cfg, tag=\"train\"):\n",
    "    sns.set_theme()\n",
    "    plt.figure()\n",
    "    plt.title(f\"{tag}ing curve on {cfg.device} if {cfg.algo_name} for {cfg.env_name}\")\n",
    "    plt.plot(rewards, label=\"rewards\")\n",
    "    plt.plot(smooth(rewards), label=\"smoothed rewards\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tee:\n",
    "    \"\"\"同时将输出写入文件和标准输出\"\"\"\n",
    "    def __init__(self, filename, mode='a'):\n",
    "        self.file = open(filename, mode, encoding='utf-8')\n",
    "        self.stdout = sys.stdout  # 备份原始标准输出\n",
    "\n",
    "    def write(self, message):\n",
    "        self.file.write(message)\n",
    "        #self.stdout.write(message)  # 同时输出到控制台\n",
    "\n",
    "    def flush(self):  # 必须实现flush方法\n",
    "        self.file.flush()\n",
    "        self.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_5351/2191230445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_agent_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"policygradient_log.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 写入覆盖模式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplot_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_5351/2689071904.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg, env, agent)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 训练并记录数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_5351/1887542982.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         '''\n\u001b[1;32m     99\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m#self.memory.clear()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = get_args()\n",
    "cfg.ckpt_path = \"ckpt/\"\n",
    "env, agent = env_agent_config(cfg)\n",
    "sys.stdout = Tee(\"policygradient_log.txt\", mode=\"w\") # 写入覆盖模式\n",
    "res_dic = train(cfg, env, agent)\n",
    "sys.stdout = sys.stdout.stdout\n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"train\")\n",
    "\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], cfg, tag=\"test\")\n",
    "#agent = PGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(algo_name='PolicyGradient', env_name='CliffWalking-v0', train_eps=400, test_eps=20, input_dim=18, gamma=0.9, lr=0.1, device='cpu', seed=8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
