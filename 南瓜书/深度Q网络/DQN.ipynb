{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度Q网络\n",
    "\n",
    "表格型方法用表格的形式存储价值函数V(s)或动作价值函数Q(s,a)，但这样的方法局限性很大。使用Q网络近似状态价值函数可以使用一个函数直接拟合状态价值函数，省去了表格存储和检索的步骤。而且还能够对**连续**的状态空间建模。DQN的主要改动有三点：\n",
    "- 使用深度神经网络替代Q表格；\n",
    "- 使用了经验回放（Replay Buffer）：使用历史数据训练，比使用一次就扔掉大大提高了样本使用效率。还可以**减少样本之间的相关性**，原则上获取经验阶段和学习阶段是分开的，原来时序的训练数据有可能是不稳定的，打乱之后再学习有助于提高训练的稳定性，和深度学习中划分数据集时打乱样本是一个道理。\n",
    "- 使用了两个网络：**策略网络**和**目标网络**，策略网络用来预测，目标网络用来计算目标值。每隔若干步才把每步更新的策略网络参数复制给目标网络，这样做也是为了稳定训练，避免Q值的发散。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import math\n",
    "import random\n",
    "import gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型\n",
    "使用一个三层MLP来构建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim=256):\n",
    "        '''初始化Q网络为全连接网络\n",
    "        '''\n",
    "        super(QNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义经验回放\n",
    "经验回放是有一定容量的，只有存储一定的transition网络才会更新，否则就退回了之前的逐步更新。经验回放需要包含两个功能或方法：\n",
    "- push：将新的transition添加到经验回放中，满了就把最开始放进去的样本挤掉，因此推荐用队列来写；\n",
    "- sample：随机采样出一个或者若干样本提供给DQN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self, transitions):\n",
    "        self.buffer.append(transitions)\n",
    "    \n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer): # 如果批量大小大于经验回放的容量 则取全部经验回放容量\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential: # 顺序采样\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else: # 随机采样\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "        \n",
    "    def clear(self):\n",
    "        '''清空经验回放\n",
    "        '''\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN优化算法\n",
    "\n",
    "基于梯度下降来实现网络的优化：\n",
    "$$\n",
    "\\theta_i \\leftarrow \\theta_i-\\lambda \\nabla_{\\theta_i} L_i (\\theta_i)\n",
    "$$\n",
    "其中$\\theta$就是神经网络的参数。对于损失函数的实现，在DQN中损失设计比较简单：\n",
    "$$\n",
    "L(\\theta)=(y_i-Q(s_i,a_i;\\theta))^2\n",
    "$$\n",
    "这里的$y_i$是目标值，$Q(s_i,a_i;\\theta)$是当前状态下的Q值。设个损失在深度学习汇总通常称为均方误差损失mesloss。$y_i$在DQN中一般表示如下：\n",
    "$$\n",
    "y_{i}= \\begin{cases}r_{i} & \\text {对于终止状态} s_{i+1} \\\\ r_{i}+\\gamma \\max _{a^{\\prime}} Q\\left(s_{i+1}, a^{\\prime} ; \\theta\\right) & \\text {对于非终止状态} s_{i+1}\\end{cases}\n",
    "$$\n",
    "该公式的意思就是将下一个状态对应的最大Q值作为实际值（因为实际值通常不能直接求得，智能近似）这种做法实际上是一种近似，可能会导致过估计等问题。也有一些改善方法具体可在后面的改进DQN算法中给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, model, memory, cfg):\n",
    "        self.n_states = cfg.n_states\n",
    "        self.n_actions = cfg.n_actions\n",
    "        self.device = torch.device(cfg.device)\n",
    "        self.gamma = cfg.gamma # 奖励的折扣因子\n",
    "\n",
    "        # e-greedy策略相关参数\n",
    "        self.sample_count = 0 # 用于epsilon的衰减计数\n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.epsilon_start = cfg.epsilon_start\n",
    "        self.epsilon_end = cfg.epsilon_end\n",
    "        self.epsilon_decay = cfg.epsilon_decay\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.policy_net = model.to(self.device)\n",
    "        if cfg.if_load_ckpt:\n",
    "            self.policy_net.load_state_dict(torch.load(cfg.ckpt_dir + \"/DQN_eps20.pth\"))\n",
    "            print(\"模型加载成功\")\n",
    "        self.target_net = model.to(self.device)\n",
    "\n",
    "        # 复制参数道目标网络\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n",
    "        self.memory = memory\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        '''采样动作\n",
    "        使用贪婪策略根据当前状态采样动作\n",
    "        '''\n",
    "        self.sample_count += 1\n",
    "        # epsilon-greedy策略\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay)\n",
    "        if random.random() > self.epsilon:\n",
    "            # 以概率(1-self.epsilon)选择最优动作 开始很保守 后来大胆探索\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.policy_net(state)\n",
    "                action = q_values.max(1)[1].item() # 最大值对应的索引 也就是动作值\n",
    "        else:\n",
    "            # 以概率self.epsilon 随机选择动作(探索)\n",
    "            action = random.randrange(self.n_actions)\n",
    "        return action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_action(self, state):\n",
    "        '''预测动作\n",
    "        '''\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.policy_net(state)\n",
    "        action = q_values.max(1)[1].item() # 最大值对应的索引 也就是动作值\n",
    "        return action\n",
    "    \n",
    "    def update(self,):\n",
    "        if len(self.memory) < self.batch_size: # 当经验池中的数据量小于batch_size时，不更新参数\n",
    "            return\n",
    "        # 从经验池中随机采样batch_size条数据\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n",
    "        # 将数据转换为tensor\n",
    "        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float)\n",
    "        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)\n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float)\n",
    "        done_batch = torch.tensor(np.float32(done_batch), device=self.device)\n",
    "\n",
    "        q_values = self.policy_net(state_batch).gather(1, index=action_batch) # 获取q值 取一个维度上的q值\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach() # 获取下一个状态的q值\n",
    "\n",
    "        # 计算期望的Q值 对于终止状态 此时done_batch[i]=1 对应的expected_q_values[i] = reward_batch[i]\n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1)) # 计算均方根损失\n",
    "\n",
    "        # 优化更新模型\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip防止梯度爆炸\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    '''训练\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    print(f\"环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}\")\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0 # 记录一轮回合内的奖励\n",
    "        ep_step = 0 # 记录每一轮的交互次数\n",
    "        state = env.reset() # 重置环境 返回初始状态\n",
    "        for _ in range(cfg.ep_max_steps):\n",
    "            ep_step += 1\n",
    "            action = agent.sample_action(state) # 选择动作\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.memory.push((state, action, reward, next_state, done)) # 保存transition到经验池中\n",
    "            state = next_state\n",
    "            agent.update()\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        if (i_ep+1) % cfg.target_update == 0: # 智能体目标网络更新\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "            #print(\"更新目标网络.\")\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        if (i_ep+1) % 10 == 0:\n",
    "            print(f\"回合: {i_ep+1}/{cfg.train_eps}, 奖励: {ep_reward:.2f}, 步数: {ep_step}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    print(\"训练完成!\")\n",
    "    env.close()\n",
    "    return {\"rewards\": rewards}\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    print(f\"测试环境: {cfg.env_name}, 算法: {cfg.algo_name}, 设备: {cfg.device}\")\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        ep_reward = 0\n",
    "        ep_step = 0\n",
    "        state = env.reset()\n",
    "        for _ in range(cfg.ep_max_steps):\n",
    "            ep_step += 1\n",
    "            action = agent.predict_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合: {i_ep+1}/{cfg.test_eps}, 奖励: {ep_reward:.2f}\")\n",
    "    print(\"完成测试！\")\n",
    "    env.close()\n",
    "    return {\"rewards\": rewards}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg.env_name)\n",
    "    if cfg.seed != 0:\n",
    "        all_seed(env, cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    cfg.n_states = n_states\n",
    "    cfg.n_actions = n_actions\n",
    "    if torch.cuda.is_available():\n",
    "        cfg.device = torch.device(\"cuda\")\n",
    "        print(\"使用GPU\")\n",
    "    else:\n",
    "        cfg.device = torch.device(\"cpu\")\n",
    "        print(\"使用CPU\")\n",
    "    agent = DQN(model=QNet(n_states=cfg.n_states, n_actions=cfg.n_actions),\n",
    "                memory=ReplayBuffer(capacity=cfg.memory_capacity),\n",
    "                cfg=cfg)\n",
    "    return env, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"配置参数\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"hyperpaparameter\")\n",
    "    parser.add_argument(\"--algo_name\", default=\"DQN\", type=str, help=\"算法名称\")\n",
    "    parser.add_argument(\"--env_name\", default=\"CartPole-v0\", type=str, help=\"环境名称\")\n",
    "    parser.add_argument(\"--n_states\", default=4, type=int, help=\"状态维度\")\n",
    "    parser.add_argument(\"--n_actions\", default=2, type=int, help=\"动作维度\")\n",
    "    parser.add_argument(\"--train_eps\", default=200, type=int, help=\"训练回合数\")\n",
    "    parser.add_argument(\"--test_eps\", default=20, type=int, help=\"测试回合数\")\n",
    "    parser.add_argument(\"--ep_max_steps\", default=100000, type=int, help=\"每个回合的最大步数\")\n",
    "\n",
    "    parser.add_argument(\"--if_load_ckpt\", default=False, type=bool, help=\"是否加载模型\")\n",
    "    parser.add_argument(\"--ckpt_dir\", default=\"ckpt/\", type=str, help=\"模型存储路径\")\n",
    "\n",
    "    parser.add_argument(\"--gamma\", default=0.95, type=float, help=\"折扣因子\")\n",
    "    parser.add_argument(\"--epsilon_start\", default=0.95, type=float, help=\"epsilon的初始值\")\n",
    "    parser.add_argument(\"--epsilon_end\", default=0.01, type=float, help=\"epsilon的终止值\")\n",
    "    parser.add_argument(\"--epsilon_decay\", default=500, type=int, help=\"epsilon的衰减值\")\n",
    "    parser.add_argument(\"--lr\", default=0.0001, type=float, help=\"学习率\")\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", default=64, type=int, help=\"batch_size\")\n",
    "    parser.add_argument(\"--target_update\", default=4, type=int, help=\"目标网络更新频率\")\n",
    "    parser.add_argument(\"--hidden_dim\", default=256, type=int, help=\"隐藏层维度\")\n",
    "    parser.add_argument(\"--memory_capacity\", default=100000, type=int, help=\"经验池容量\")\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", type=str, help=\"cpu或者gpu\")\n",
    "    parser.add_argument(\"--seed\", default=10, type=int, help=\"随机种子\")\n",
    "\n",
    "    _args = parser.parse_args([])\n",
    "    args = {**vars(_args)} # 将args转换为字典\n",
    "    # 打印超参数\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(\"\".join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k, v, str(type(v))))\n",
    "    print(\"\".join([\"=\"]*80))\n",
    "    return _args\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,cfg, tag='train'):\n",
    "    ''' 画图\n",
    "    '''\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{tag}ing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "================================================================================\n",
      "        Name        \t       value        \t        Type        \n",
      "     algo_name      \t        DQN         \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v0     \t   <class 'str'>    \n",
      "      n_states      \t         4          \t   <class 'int'>    \n",
      "     n_actions      \t         2          \t   <class 'int'>    \n",
      "     train_eps      \t        1000        \t   <class 'int'>    \n",
      "      test_eps      \t         20         \t   <class 'int'>    \n",
      "    ep_max_steps    \t       100000       \t   <class 'int'>    \n",
      "    if_load_ckpt    \t         0          \t   <class 'bool'>   \n",
      "      ckpt_dir      \t       ckpt/        \t   <class 'str'>    \n",
      "       gamma        \t        0.95        \t  <class 'float'>   \n",
      "   epsilon_start    \t        0.95        \t  <class 'float'>   \n",
      "    epsilon_end     \t        0.01        \t  <class 'float'>   \n",
      "   epsilon_decay    \t        500         \t   <class 'int'>    \n",
      "         lr         \t       0.0001       \t  <class 'float'>   \n",
      "     batch_size     \t         64         \t   <class 'int'>    \n",
      "   target_update    \t         4          \t   <class 'int'>    \n",
      "     hidden_dim     \t        256         \t   <class 'int'>    \n",
      "  memory_capacity   \t       100000       \t   <class 'int'>    \n",
      "       device       \t        cpu         \t   <class 'str'>    \n",
      "        seed        \t         10         \t   <class 'int'>    \n",
      "================================================================================\n",
      "状态空间维度：4，动作空间维度：2\n",
      "使用CPU\n",
      "开始训练！\n",
      "环境：CartPole-v0, 算法：DQN, 设备：cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/envs/registration.py:592: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合: 10/1000, 奖励: 12.00, 步数: 12, Epsilon: 0.65\n",
      "回合: 20/1000, 奖励: 9.00, 步数: 9, Epsilon: 0.48\n",
      "回合: 30/1000, 奖励: 15.00, 步数: 15, Epsilon: 0.38\n",
      "回合: 40/1000, 奖励: 11.00, 步数: 11, Epsilon: 0.31\n",
      "回合: 50/1000, 奖励: 12.00, 步数: 12, Epsilon: 0.25\n",
      "回合: 60/1000, 奖励: 40.00, 步数: 40, Epsilon: 0.14\n",
      "回合: 70/1000, 奖励: 74.00, 步数: 74, Epsilon: 0.04\n",
      "回合: 80/1000, 奖励: 200.00, 步数: 200, Epsilon: 0.01\n",
      "回合: 90/1000, 奖励: 200.00, 步数: 200, Epsilon: 0.01\n",
      "回合: 100/1000, 奖励: 200.00, 步数: 200, Epsilon: 0.01\n",
      "回合: 110/1000, 奖励: 200.00, 步数: 200, Epsilon: 0.01\n",
      "回合: 120/1000, 奖励: 200.00, 步数: 200, Epsilon: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_79964/1755142973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplot_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rewards\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_79964/1399193663.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg, env, agent)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mep_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mep_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 选择动作\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 保存transition到经验池中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jm/bmm2rb4j57z3mk6xq6hzgf6m0000gn/T/ipykernel_79964/1824558638.py\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# epsilon-greedy策略\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_end\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_start\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# 以概率(1-self.epsilon)选择最优动作 开始很保守 后来大胆探索\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()\n",
    "env, agent = env_agent_config(cfg)\n",
    "\n",
    "# 训练\n",
    "res_dic = train(cfg, env, agent)\n",
    "plot_rewards(res_dic[\"rewards\"], cfg, tag=\"train\")\n",
    "\n",
    "# 测试\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic[\"rewards\"], cfg, tag=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(algo_name='DQN', env_name='CartPole-v0', n_states=4, n_actions=2, train_eps=1000, test_eps=20, ep_max_steps=100000, if_load_ckpt=False, ckpt_dir='ckpt/', gamma=0.95, epsilon_start=0.95, epsilon_end=0.01, epsilon_decay=500, lr=0.0001, batch_size=64, target_update=4, hidden_dim=256, memory_capacity=100000, device=device(type='cpu'), seed=10)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
