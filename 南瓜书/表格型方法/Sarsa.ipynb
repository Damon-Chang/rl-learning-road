{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa\n",
    "Sarsa算法是表格型方法的一种。自举式更新，睡觉哦on-policy的方法。通过结合时序差分和蒙特卡洛方法估计价值函数。\n",
    "- 蒙特卡洛\n",
    "$$ \n",
    "V(s_t)=V(s_t)+\\alpha (G_t - V(s_t))\n",
    "$$\n",
    "- 时序差分\n",
    "假设做的是n阶时序差分，$n=\\infty$ 表示蒙特卡洛方法。\n",
    "$$\n",
    "G_t^{(n)}=r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+3}+...+\\gamma^{n-1}r_{t+n}+\\gamma^n V(s_{t+n})\n",
    "$$\n",
    "$$\n",
    "TD(n):V(s_t)=V(s_t)+\\alpha (G_t^{(n)} - V(s_t)) \n",
    "$$\n",
    "- Sarsa\n",
    "将原本的**单步**时序差分方法更新V的过程，变成更新Q，即\n",
    "$$\n",
    "Q(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha (r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))\n",
    "$$\n",
    "Sarsa直接更新Q-table，得到Q-table之后就可以更新策略。每次更新需要知道当前状态、当前动作、奖励、下一步状态、下一步动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import dill\n",
    "import gym\n",
    "import turtle\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Sarsa算法类\n",
    "输入为动作数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "    def __init__(self, n_actions, cfg):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = cfg.lr\n",
    "        self.gamma = cfg.gamma # 折扣因子\n",
    "        self.sample_count = 0 # 记录已经采样的次数\n",
    "        self.epsilon_start = cfg.epsilon_start # 贪心策略epsilon的初始值0.95\n",
    "        self.epsilon_end = cfg.epsilon_end # 贪心策略epsilon的终止值0.01\n",
    "        self.epsilon_decay = cfg.epsilon_decay # 贪心策略epsilon的衰减率\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions)) # Q table\n",
    "    \n",
    "    def sample(self, state):\n",
    "        '''从当前状态采样动作'''\n",
    "        self.sample_count += 1\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay) # 指数衰减的贪心策略epsilon\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        action_probs = np.ones(self.n_actions, dtype=float) * self.epsilon / self.n_actions\n",
    "        action_probs[best_action] += (1.0 - self.epsilon) # 保证概率只和为1\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        return action\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        '''更新Q表格'''\n",
    "        Q_predict = self.Q[state, action]\n",
    "        if done:\n",
    "            Q_target = reward\n",
    "        else:\n",
    "            Q_target = reward + self.gamma * self.Q[next_state][next_action] # 与Qlearning不同，Sarsa用下一步的动作对应的Q值进行更新\n",
    "        self.Q[state][action] += self.lr * (Q_target - Q_predict)\n",
    "    \n",
    "    def save(self, path):\n",
    "        '''把Q表格的数据保存到本地文件'''\n",
    "        #import dill\n",
    "        torch.save(\n",
    "            obj=self.Q,\n",
    "            f=path+\"sarsa_model.pkl\",\n",
    "            pickle_protocol=dill # dill是pickle的扩展库，支持更复杂的对象序列化\n",
    "            )\n",
    "        \n",
    "    def load(self, path):\n",
    "        '''从文件中读取数据到Q表格\n",
    "        '''\n",
    "        self.Q = torch.loaf(f=path+\"sarsa_model.pkl\", pickle_protocol=dill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练\n",
    "和Q learning差别不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    '''\n",
    "    - cfg: 配置参数\n",
    "    - env: 环境\n",
    "    - agent: 算法\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    print(f'环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}')\n",
    "    rewards = [] # 记录奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward = 0 # 一轮episode的reward\n",
    "        state = env.reset(seed = cfg.seed) # 重置环境, 重新开一局（即开始新的一个episode）\n",
    "        action = agent.sample_action(state)\n",
    "        while True:\n",
    "            action = agent.sample_action(state) # 根据算法采样一个动作\n",
    "            next_state, reward, done, _ = env.step(action) # 与环境进行一个交互\n",
    "            next_action = agent.sample_action(next_state)\n",
    "            agent.update(state, action, reward, next_state, next_action, done) # Sarsa算法更新\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.1f}，Epsilon：{agent.epsilon:.3f}\")\n",
    "    print(\"完成训练！\")\n",
    "    return {\"rewards\": rewards}\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    print(f\"环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}\")\n",
    "    rewards = [] # 记录所有回合的episode奖励\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        ep_reward = 0 # 一轮episode的reward\n",
    "        state = env.reset(seed = cfg.seed)\n",
    "        while True:\n",
    "            action = agent.predict_action(state) # 根据算法选择一个动作\n",
    "            next_state, reward, done, _ = env.step(action) # 与环境进行一个交互\n",
    "            state = next_state # 更新状态\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.1f}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {\"rewards\": rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "#import numpy as np\n",
    "#import turtle\n",
    "\n",
    "#from envs.wrapper import FrozenLakeWrapper\n",
    "\n",
    "def GridWorld(gridmap=None, is_slippery=False):\n",
    "    if gridmap is None:\n",
    "        gridmap = ['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
    "    env = gym.make(\"FrozenLake-v0\", desc=gridmap, is_slippery=is_slippery)\n",
    "    env = FrozenLakeWrapper(env)\n",
    "    return env\n",
    "\n",
    "#'''\n",
    "class FrozenLakeWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''定义一个Wrapper修饰环境'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.max_y = env.desc.shape[0]\n",
    "        self.max_x = env.desc.shape[1]\n",
    "        self.t = None\n",
    "        self.unit = 50\n",
    "    \n",
    "    def draw_box(self, x, y, fillcolor='', line_color='gray'):\n",
    "        # 绘制盒子\n",
    "        self.t.up()\n",
    "        self.t.goto(x * self.unit, y * self.unit)\n",
    "        self.t.color(line_color)\n",
    "        self.t.fillcolor(fillcolor)\n",
    "        self.t.setheading(90)\n",
    "        self.t.down()\n",
    "        self.t.begin_fill()\n",
    "        for _ in range(4):\n",
    "            self.t.forward(self.unit)\n",
    "            self.t.right(90)\n",
    "        self.t.end_fill()\n",
    "\n",
    "    def move_player(self, x, y):\n",
    "        # 移动玩家\n",
    "        self.t.up()\n",
    "        self.t.setheading(90)\n",
    "        self.t.fillcolor('red')\n",
    "        self.t.goto((x + 0.5) * self.unit, (y + 0.5) * self.unit)\n",
    "    \n",
    "    def render(self):\n",
    "        # 渲染\n",
    "        if self.t == None:\n",
    "            self.t = turtle.Turtle()\n",
    "            self.wn = turtle.Screen()\n",
    "            self.wn.setup(self.unit * self.max_x + 100,\n",
    "                          self.unit * self.max_y + 100)\n",
    "            self.wn.setworldcoordinates(0, 0, self.unit * self.max_x,\n",
    "                                        self.unit * self.max_y)\n",
    "            self.t.shape('circle')\n",
    "            self.t.width(2)\n",
    "            self.t.speed(0)\n",
    "            self.t.color('gray')\n",
    "            for i in range(self.desc.shape[0]):\n",
    "                for j in range(self.desc.shape[1]):\n",
    "                    x = j\n",
    "                    y = self.max_y - 1 - i\n",
    "                    if self.desc[i][j] == b'S':  # Start\n",
    "                        self.draw_box(x, y, 'white')\n",
    "                    elif self.desc[i][j] == b'F':  # Frozen ice\n",
    "                        self.draw_box(x, y, 'white')\n",
    "                    elif self.desc[i][j] == b'G':  # Goal\n",
    "                        self.draw_box(x, y, 'yellow')\n",
    "                    elif self.desc[i][j] == b'H':  # Hole\n",
    "                        self.draw_box(x, y, 'black')\n",
    "                    else:\n",
    "                        self.draw_box(x, y, 'white')\n",
    "            self.t.shape('turtle')\n",
    "\n",
    "        x_pos = self.s % self.max_x\n",
    "        y_pos = self.max_y - 1 - int(self.s / self.max_x)\n",
    "        self.move_player(x_pos, y_pos)\n",
    "\n",
    "class CliffWalkingWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.t = None\n",
    "        self.unit = 50\n",
    "        self.max_x = 12\n",
    "        self.max_y = 4\n",
    "\n",
    "    def draw_x_line(self, y, x0, x1, color='gray'):\n",
    "        assert x1 > x0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(0)\n",
    "        self.t.up()\n",
    "        self.t.goto(x0, y)\n",
    "        self.t.down()\n",
    "        self.t.forward(x1 - x0)\n",
    "\n",
    "    def draw_y_line(self, x, y0, y1, color='gray'):\n",
    "        assert y1 > y0\n",
    "        self.t.color(color)\n",
    "        self.t.setheading(90)\n",
    "        self.t.up()\n",
    "        self.t.goto(x, y0)\n",
    "        self.t.down()\n",
    "        self.t.forward(y1 - y0)\n",
    "\n",
    "    def draw_box(self, x, y, fillcolor='', line_color='gray'):\n",
    "        self.t.up()\n",
    "        self.t.goto(x * self.unit, y * self.unit)\n",
    "        self.t.color(line_color)\n",
    "        self.t.fillcolor(fillcolor)\n",
    "        self.t.setheading(90)\n",
    "        self.t.down()\n",
    "        self.t.begin_fill()\n",
    "        for i in range(4):\n",
    "            self.t.forward(self.unit)\n",
    "            self.t.right(90)\n",
    "        self.t.end_fill()\n",
    "\n",
    "    def move_player(self, x, y):\n",
    "        self.t.up()\n",
    "        self.t.setheading(90)\n",
    "        self.t.fillcolor('red')\n",
    "        self.t.goto((x + 0.5) * self.unit, (y + 0.5) * self.unit)\n",
    "\n",
    "    def render(self):\n",
    "        if self.t == None:\n",
    "            self.t = turtle.Turtle()\n",
    "            self.wn = turtle.Screen()\n",
    "            self.wn.setup(self.unit * self.max_x + 100,\n",
    "                          self.unit * self.max_y + 100)\n",
    "            self.wn.setworldcoordinates(0, 0, self.unit * self.max_x,\n",
    "                                        self.unit * self.max_y)\n",
    "            self.t.shape('circle')\n",
    "            self.t.width(2)\n",
    "            self.t.speed(0)\n",
    "            self.t.color('gray')\n",
    "            for _ in range(2):\n",
    "                self.t.forward(self.max_x * self.unit)\n",
    "                self.t.left(90)\n",
    "                self.t.forward(self.max_y * self.unit)\n",
    "                self.t.left(90)\n",
    "            for i in range(1, self.max_y):\n",
    "                self.draw_x_line(\n",
    "                    y=i * self.unit, x0=0, x1=self.max_x * self.unit)\n",
    "            for i in range(1, self.max_x):\n",
    "                self.draw_y_line(\n",
    "                    x=i * self.unit, y0=0, y1=self.max_y * self.unit)\n",
    "\n",
    "            for i in range(1, self.max_x - 1):\n",
    "                self.draw_box(i, 0, 'black')\n",
    "            self.draw_box(self.max_x - 1, 0, 'yellow')\n",
    "            self.t.shape('turtle')\n",
    "\n",
    "        x_pos = self.s % self.max_x\n",
    "        y_pos = self.max_y - 1 - int(self.s / self.max_x)\n",
    "        self.move_player(x_pos, y_pos)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建环境和智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, seed=1):\n",
    "    '''创建环境和智能体\n",
    "    Args:\n",
    "        cfg: 配置参数\n",
    "        seed: 随机种子\n",
    "    Returns:\n",
    "        env: 环境\n",
    "        agent: 智能体\n",
    "    '''\n",
    "    env = gym.make(cfg.env_name)\n",
    "    env = CliffWalkingWrapper(env)\n",
    "    env.seed(seed)\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态数：{n_states}, 动作数：{n_actions}\")\n",
    "    agent = Sarsa(n_actions, cfg)\n",
    "    return env, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {1: array([0., 0., 0.]),\n",
       "             5: array([0., 0., 0.]),\n",
       "             'da': array([0., 0., 0.])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import datetime\n",
    "#import argparse\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"设置参数\n",
    "    \"\"\"\n",
    "    curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # 获取当前时间\n",
    "    parser = argparse.ArgumentParser(description=\"hyperpaparameter\")\n",
    "    parser.add_argument(\"--algo_name\", default=\"Sarsa\", type=str, help=\"算法名称\")\n",
    "    parser.add_argument(\"--env_name\", default=\"CliffWalking-v0\", type=str, help=\"环境名称\")\n",
    "    parser.add_argument(\"--train_eps\", default=400, type=int, help=\"训练回合数\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
