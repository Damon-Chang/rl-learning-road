{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3算法\n",
    "强化学习中的3步时序差分（TD）算法是一种用于**估计值函数的方法**，以下是其重点介绍：\n",
    "\n",
    "### 算法原理\n",
    "3 步 TD 算法基于贝尔曼方程，利用当前及后续有限步的奖励和状态信息来更新值函数估计，结合了蒙特卡洛方法和动态规划的思想，通过引导式的估计来逐步逼近真实值函数。\n",
    "\n",
    "### 关键公式\n",
    "- **回报计算**：对于3步TD算法，回报$G_{t}$的计算为：\n",
    "$$\n",
    "G_{t}=R_{t + 1}+\\gamma R_{t + 2}+\\gamma^{2} R_{t + 3}+\\gamma^{3} V(S_{t + 3})\n",
    "$$\n",
    "其中$R_{t + i}$是在时刻$t + i$获得的奖励，$\\gamma$是折扣因子，$V(S_{t + 3})$是时刻$t + 3$状态的估计值函数。\n",
    "- **值函数更新**：值函数$V(S_{t})$的更新公式为：\n",
    "$$\n",
    "V(S_{t}) \\leftarrow V(S_{t})+\\alpha(G_{t}-V(S_{t}))\n",
    "$$\n",
    "其中$\\alpha$是学习率，用于控制更新的步长。\n",
    "\n",
    "### 算法流程\n",
    "1. 初始化值函数$V(s)$和所有状态-动作对的访问计数。\n",
    "2. 从初始状态$S_{0}$开始，按照当前策略执行动作，收集奖励和状态转移信息。\n",
    "3. 每3步进行一次值函数更新，根据上述公式计算回报$G_{t}$并更新值函数。\n",
    "4. 重复步骤2和3，直到达到预设的迭代次数或收敛条件。\n",
    "\n",
    "3步TD算法通过有限步的信息来更新值函数，在计算效率和估计准确性之间取得了一定的平衡，适用于许多实际的强化学习问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义算法\n",
    "#### 建立Action和Critic网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim=256):\n",
    "        '''\n",
    "        初始化Actor网络为全连接网络\n",
    "        Actor网络输入状态，输出一个动作概率分布，即输出一个动作的概率\n",
    "        '''\n",
    "        self.l1 = nn.Linear(n_states, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, n_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        前向传播\n",
    "        '''\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))\n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
